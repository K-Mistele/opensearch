/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\nclient<llm> GeminiFlash25 {\n  provider google-ai\n  options {\n    model \"gemini-2.5-flash\"\n    api_key env.GOOGLE_API_KEY\n  }\n}\n\nclient<llm> GeminiPro25 {\n  provider google-ai\n  options {\n    model \"gemini-2.5-pro\"\n    api_key env.GOOGLE_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}\n",
  "create_answer.baml": "function CreateAnswer(\n    current_date: string,\n    research_topic: string,\n    summaries: string[],\n) -> string {\n    client \"google-ai/gemini-2.5-pro\"\n    prompt #\"\n        Generate a high-quality answer to the user's question based on the provided summaries.\n\n        Instructions:\n        - The current date is {{current_date}}.\n        - You are the final step of a multi-step research process, don't mention that you are the final step. \n        - You have access to all the information gathered from the previous steps.\n        - You have access to the user's question.\n        - Generate a high-quality answer to the user's question based on the provided summaries and the user's question.\n        - Include the sources you used from the Summaries in the answer correctly, use markdown format (e.g. [apnews](https://vertexaisearch.cloud.google.com/id/1-0)). THIS IS A MUST.\n\n        User Context:\n        - {{research_topic}}\n\n        Summaries:\n        {% for summary in summaries %}\n            <summary>\n            {{summary}}\n            </summary>\n        {% endfor %}\n    \"#\n}\n",
  "generate_query.baml": "\nclass GenerateQueryArgs {\n    research_topic string\n    current_date string\n    number_queries int?\n}\n/// Generate an optimized query (or several) for web research\nfunction GenerateQuery(\n    args: GenerateQueryArgs,\n) -> SearchQueryList {\n    client \"google-ai/gemini-2.5-flash\"\n    prompt #\"\n    \n        {{ _.role(\"system\")}}\n        Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\n\n        Instructions:\n        - Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\n        - Each query should focus on one specific aspect of the original question.\n        {% if args.number_queries %}\n        - Don't produce more than {{args.number_queries}} queries.\n        {% elif args.number_queries == 1 %}\n        - Generate 1 query.\n        {% else %}\n        - Generate a few queries\n        {% endif %}\n        - Queries should be diverse, if the topic is broad, generate more than 1 query to get sufficient coverage of different keywords and assumptions.\n        - Don't generate multiple similar queries, 1 is enough.\n        - Query should ensure that the most current information is gathered. The current date is {{current_date}}.\n\n        The research topic is provided between <research_topic></research_topic> XML tags below.\n        <research_topic>\n        {{ args.research_topic}}\n        </research_topic>\n\n\n        <output_format>\n        {{ ctx.output_format }}\n        </output_format>\n    \"#\n}\n\ntest NapthaAiFunding {\n    functions [GenerateQuery]\n    args {\n        args {\n            research_topic \"What is Naptha AI's most recent funding round in terms of dollars raised and the round size? (pre-seed, seed,Series A, Series B, Series C, Series D)\"\n            current_date \"06/24/2025\"\n        }\n\n    }\n}\n\ntest ConstellateAIFunding {\n    functions [GenerateQuery]\n    args {\n       args {\n            research_topic \"What is Constellate AI's most recent funding round in terms of dollars raised and the round size? (pre-seed, seed,Series A, Series B, Series C, Series D)\"\n            number_queries 10\n            current_date: \"06/24/2025\"\n       }\n    }\n}\n\ntest HumanLayerFunding {\n    functions [GenerateQuery]\n    args {\n        args {\n            research_topic \"What is Human Layer's most recent funding round in terms of dollars raised and the round size? (pre-seed, seed,Series A, Series B, Series C, Series D)\"\n            number_queries 10\n            current_date: \"06/24/2025\"\n        }\n    }\n}   \n\ntest BoundaryMLFunding {\n    functions [GenerateQuery]\n    args {\n        args {\n            research_topic \"What is BoundaryML's most recent funding round in terms of dollars raised and the round size? (pre-seed, seed,Series A, Series B, Series C, Series D)\"\n            number_queries 10\n            current_date: \"06/24/2025\"\n        }\n    }\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.90.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "reflect.baml": "function Reflect(\n    summaries: string,\n    research_topic: string,\n) -> Reflection {\n    client \"google-ai/gemini-2.5-pro\"\n    prompt #\"\n    You are an expert research assistant analyzing summaries about \"{{research_topic}}\".\n    Instructions:\n    - Identify knowledge gaps or areas that need deeper exploration and generate a follow-up query. (1 or multiple).\n    - If provided summaries are sufficient to answer the user's question, don't generate a follow-up query.\n    - If there is a knowledge gap, generate a follow-up query that would help expand your understanding.\n    - Focus on technical details, implementation specifics, or emerging trends that weren't fully covered.\n\n    Requirements:\n    - Ensure the follow-up query is self-contained and includes necessary context for web search.\n\n    Reflect carefully on the Summaries to identify knowledge gaps and produce a follow-up query. Then, produce your output following this JSON format:\n    {{ctx.output_format}}\n\n    Example:\n    ```json\n    {\n        \"is_sufficient\": true, // or false\n        \"knowledge_gap\": \"The summary lacks information about performance metrics and benchmarks\", // \"\" if is_sufficient is true\n        \"follow_up_queries\": [\"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\"] // [] if is_sufficient is true\n    }\n    ```\n\n    Summaries:\n    {% for summary in summaries %}\n        <summary>\n        {{summary}}\n        </summary>\n    {% endfor %}\n    \"#\n}",
  "types.baml": "// State management types for the research workflow\nclass SearchQueryList {\n    query string[] @description(\"A list of search queries to be used for web research.\")\n    rationale string @description(\"A brief explanation of why these queries are relevant to the research topic.\")\n}\nclass SufficientReflection {\n    isSufficient true @description(\"Whether the provided summaries are sufficient to answer the user's question\")\n}\n\nclass InsufficientReflection {\n    isSufficient false @description(\"Whether the provided summaries are sufficient to answer the user's question\")\n    knowledgeGap bool @description(\"A description of what information is missing or needs clarification\")\n    followUpQueries string[] @description(\"A list of follow-up questions to be researched to address the knowledge gap\")\n}\n\ntype Reflection = SufficientReflection | InsufficientReflection\n\nclass Message {\n    role string @description(\"The role of the message sender (e.g., 'user', 'assistant', 'system')\")\n    content string @description(\"The content of the message\")\n}\n\nclass OverallState {\n    messages Message[] @description(\"List of messages in the conversation\")\n    search_query string[] @description(\"List of search queries generated\")\n    web_research_result string[] @description(\"Results from web research\")\n    sources_gathered string[] @description(\"Sources collected during research\")\n    initial_search_query_count int @description(\"Initial count of search queries\")\n    max_research_loops int @description(\"Maximum number of research loops allowed\")\n    research_loop_count int @description(\"Current research loop count\")\n    reasoning_model string @description(\"The model used for reasoning\")\n}\n\nclass ReflectionState {\n    is_sufficient bool @description(\"Whether the current information is sufficient\")\n    knowledge_gap string @description(\"Description of the knowledge gap identified\")\n    follow_up_queries string[] @description(\"Follow-up queries to address knowledge gaps\")\n    research_loop_count int @description(\"Current research loop count\")\n    number_of_ran_queries int @description(\"Number of queries that have been executed\")\n}\n\nclass Query {\n    query string @description(\"The search query string\")\n    rationale string @description(\"Rationale for why this query is needed\")\n}\n\nclass QueryGenerationState {\n    search_query Query[] @description(\"List of generated queries with rationales\")\n}\n\nclass WebSearchState {\n    search_query string @description(\"The search query to execute\")\n    id string @description(\"Unique identifier for the search\")\n}\n\nclass SearchStateOutput {\n    running_summary string? @description(\"Final report or running summary of research\")\n} \n\nclass SearchResult {\n    url string @description(\"The URL of the search result\")\n    id string @description(\"The ID of the search result\")\n    title string @description(\"The title of the search result\")\n    highlights string[] @description(\"The highlights of the search result\")\n    highlightScores float[] @description(\"The highlight scores of the search result\")\n    text string @description(\"The text of the search result\")\n}",
  "web_search.baml": "function WebSearch(\n    current_date: string,\n    research_topic: string,\n) -> string {\n    client \"google-ai/gemini-2.5-flash\"\n    prompt #\"\n    Conduct targeted Google Searches to gather the most recent, credible information on \"{{research_topic}}\" and synthesize it into a verifiable text artifact.\n\n    Instructions:\n    - Query should ensure that the most current information is gathered. The current date is {{current_date}}.\n    - Conduct multiple, diverse searches to gather comprehensive information.\n    - Consolidate key findings while meticulously tracking the source(s) for each specific piece of information.\n    - The output should be a well-written summary or report based on your search findings. \n    - Only include the information found in the search results, don't make up any information.\n\n    Research Topic:\n    {{research_topic}}\n    \"#\n}",
}
export const getBamlFiles = () => {
    return fileMap;
}